{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95722553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86554f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, pytz\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "SP = pytz.timezone(\"America/Sao_Paulo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c887e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dotenv import load_dotenv\n",
    "#from pathlib import Path\n",
    "#load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bdf93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[^\\w]+\", \"_\", s, flags=re.UNICODE)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"col\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f98c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sep(sample_bytes: bytes) -> str:\n",
    "    # tenta deduzir separador por sniffing\n",
    "    try:\n",
    "        txt = sample_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "        dialect = csv.Sniffer().sniff(txt, delimiters=[\",\", \";\", \"\\t\", \"|\"])\n",
    "        return dialect.delimiter\n",
    "    except Exception:\n",
    "        return \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e45bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(years: list[int]):\n",
    "    bucket_name = os.environ[\"BUCKET\"]\n",
    "    raw_prefix = os.environ.get(\"RAW_PREFIX\", \"raw/bps/\")\n",
    "    stg_prefix = os.environ.get(\"STG_PREFIX\", \"stg/bps/\")\n",
    "\n",
    "    ingest_date_filter = os.environ.get(\"INGEST_DATE\")\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Lista CSVs no RAW\n",
    "    blobs = list(client.list_blobs(bucket, prefix=raw_prefix))\n",
    "    csv_blobs = [b for b in blobs if b.name.lower().endswith(\".csv\")]\n",
    "\n",
    "    if not csv_blobs:\n",
    "        raise RuntimeError(f\"Nenhum .csv encontrado em gs://{bucket_name}/{raw_prefix}\")\n",
    "\n",
    "    def parse_year_and_ingest(name: str):\n",
    "        my = re.search(r\"year=(20\\d{2})\", name)\n",
    "        mi = re.search(r\"ingest_date=(\\d{4}-\\d{2}-\\d{2})\", name)\n",
    "        y = int(my.group(1)) if my else None\n",
    "        ingest = mi.group(1) if mi else None\n",
    "        return y, ingest\n",
    "\n",
    "    selected = []\n",
    "    for b in csv_blobs:\n",
    "        y, ingest = parse_year_and_ingest(b.name)\n",
    "        if y is None or ingest is None:\n",
    "            continue\n",
    "        if years and y not in years:\n",
    "            continue\n",
    "        if ingest_date_filter and ingest != ingest_date_filter:\n",
    "            continue\n",
    "        selected.append((y, ingest, b))\n",
    "\n",
    "    if not selected:\n",
    "        raise RuntimeError(\n",
    "            f\"Nenhum CSV selecionado em gs://{bucket_name}/{raw_prefix} \"\n",
    "            f\"para years={years} ingest_date={ingest_date_filter or '*'}\"\n",
    "        )\n",
    "\n",
    "    for y, ingest, b in sorted(selected, key=lambda t: (t[0], t[1], t[2].name)):\n",
    "        print(f\"\\n==> Processando RAW: gs://{bucket_name}/{b.name}\")\n",
    "\n",
    "        base_name = os.path.basename(b.name)\n",
    "        stem = Path(base_name).stem\n",
    "\n",
    "        local_csv = f\"/tmp/{stem}__{y}__{ingest}.csv\"\n",
    "        b.download_to_filename(local_csv)\n",
    "\n",
    "        # Detecta separador\n",
    "        with open(local_csv, \"rb\") as fh:\n",
    "            sample = fh.read(4096)\n",
    "        sep = detect_sep(sample)\n",
    "\n",
    "        def make_reader(enc: str):\n",
    "            return pd.read_csv(\n",
    "                local_csv,\n",
    "                sep=sep,\n",
    "                dtype=str,\n",
    "                encoding=enc,\n",
    "                chunksize=200_000,\n",
    "                low_memory=False,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            reader = make_reader(\"utf-8\")\n",
    "            next(iter(reader))\n",
    "            reader = make_reader(\"utf-8\")\n",
    "        except Exception:\n",
    "            reader = make_reader(\"latin1\")\n",
    "\n",
    "        part = 0\n",
    "        for chunk in reader:\n",
    "            chunk.columns = [snake(c) for c in chunk.columns]\n",
    "\n",
    "            # metadados\n",
    "            chunk[\"source_year\"] = str(y)\n",
    "            chunk[\"ingest_date\"] = ingest\n",
    "            chunk[\"source_object\"] = f\"gs://{bucket_name}/{b.name}\"\n",
    "            chunk[\"load_ts_utc\"] = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "            local_parquet = f\"/tmp/bps_{y}_{ingest}_part{part:05d}.parquet\"\n",
    "\n",
    "            chunk.to_parquet(\n",
    "                local_parquet,\n",
    "                index=False,\n",
    "                engine=\"pyarrow\",\n",
    "                compression=\"snappy\",\n",
    "            )\n",
    "\n",
    "            # üëá agora sem source_file na parti√ß√£o\n",
    "            gcs_key = f\"{stg_prefix}year={y}/ingest_date={ingest}/part-{part:05d}.parquet\"\n",
    "\n",
    "            bucket.blob(gcs_key).upload_from_filename(local_parquet)\n",
    "\n",
    "            print(f\"   - OK: gs://{bucket_name}/{gcs_key} (rows={len(chunk)})\")\n",
    "\n",
    "            os.remove(local_parquet)\n",
    "            part += 1\n",
    "\n",
    "        os.remove(local_csv)\n",
    "\n",
    "        print(f\"==> Conclu√≠do: year={y} ingest_date={ingest} parts={part}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbf31fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Processando RAW: gs://rq-pharma-raw-rq-pharma-data-lab-26k9/raw/bps/year=2024/ingest_date=2026-02-13/2024.csv\n",
      "   - OK: gs://rq-pharma-raw-rq-pharma-data-lab-26k9/stg/bps/year=2024/ingest_date=2026-02-13/part-00000.parquet (rows=20512)\n",
      "==> Conclu√≠do: year=2024 ingest_date=2026-02-13 parts=1\n",
      "\n",
      "==> Processando RAW: gs://rq-pharma-raw-rq-pharma-data-lab-26k9/raw/bps/year=2025/ingest_date=2026-02-13/2025.csv\n",
      "   - OK: gs://rq-pharma-raw-rq-pharma-data-lab-26k9/stg/bps/year=2025/ingest_date=2026-02-13/part-00000.parquet (rows=2474)\n",
      "==> Conclu√≠do: year=2025 ingest_date=2026-02-13 parts=1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    years = [int(a) for a in sys.argv[1:] if re.fullmatch(r\"20\\d{2}\", a)]\n",
    "    main(years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
