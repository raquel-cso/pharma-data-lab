{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "from google.cloud import storage, bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dotenv import load_dotenv\n",
    "#from pathlib import Path\n",
    "#load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796fe904",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "BUCKET = os.environ.get(\"BUCKET\")\n",
    "\n",
    "SRC_PREFIX = os.environ.get(\"SRC_PREFIX\")\n",
    "SILVER_TARGET = os.environ.get(\"SILVER_TARGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329935b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se quiser limitar:\n",
    "YEARS = [y for y in os.environ.get(\"YEARS\", \"\").split(\",\") if y.strip()]\n",
    "INGEST_DATE = os.environ.get(\"INGEST_DATE\")  # opcional (ex: 2026-02-13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_parquets():\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    blobs = client.list_blobs(BUCKET, prefix=SRC_PREFIX)\n",
    "    files = []\n",
    "    for b in blobs:\n",
    "        if not b.name.endswith(\".parquet\"):\n",
    "            continue\n",
    "        # filtra por year=\n",
    "        m = re.search(r\"year=(20\\d{2})\", b.name)\n",
    "        year = m.group(1) if m else None\n",
    "        if YEARS and (year not in YEARS):\n",
    "            continue\n",
    "        if INGEST_DATE and (f\"ingest_date={INGEST_DATE}\" not in b.name):\n",
    "            continue\n",
    "        files.append(f\"gs://{BUCKET}/{b.name}\")\n",
    "    return sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc23bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_int(series):\n",
    "    return pd.to_numeric(series.astype(str).str.replace(r\"[^0-9]\", \"\", regex=True), errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def _to_num(series):\n",
    "    # aceita \"1234.56\" e \"1.234,56\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.str.replace(\".\", \"\", regex=False)          # remove milhar\n",
    "    s = s.str.replace(\",\", \".\", regex=False)         # vírgula -> ponto\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _to_dt(series):\n",
    "    # padrão que você mostrou: 2024/01/01 00:00:00.000\n",
    "    s = series.astype(str).str.strip()\n",
    "    dt = pd.to_datetime(s, format=\"%Y/%m/%d %H:%M:%S.%f\", errors=\"coerce\")\n",
    "    if dt.isna().all():\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99604ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parquet_files = _list_parquets()\n",
    "    if not parquet_files:\n",
    "        raise RuntimeError(\"Nenhum parquet encontrado com os filtros atuais.\")\n",
    "\n",
    "    print(f\"Arquivos parquet: {len(parquet_files)} (ex.: {parquet_files[0]})\")\n",
    "\n",
    "    # lê tudo (MVP). Se crescer, a gente muda pra leitura por dataset/partição.\n",
    "    dfs = []\n",
    "    for p in parquet_files:\n",
    "        import gcsfs\n",
    "        import pyarrow.parquet as pq\n",
    "\n",
    "        fs = gcsfs.GCSFileSystem(project=PROJECT_ID)\n",
    "\n",
    "        with fs.open(p, \"rb\") as f:\n",
    "            table = pq.read_table(f)          # lê como arquivo único (não como dataset)\n",
    "        df = table.to_pandas()\n",
    "\n",
    "        # garante consistência\n",
    "        if \"ingest_date\" in df.columns:\n",
    "            df[\"ingest_date\"] = df[\"ingest_date\"].astype(str)\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"Linhas lidas:\", len(df))\n",
    "\n",
    "    # --- normalizações/tipagens (Silver) ---\n",
    "    df[\"uf\"] = df[\"uf\"].astype(str).str.strip().str.upper()\n",
    "    df.loc[df[\"uf\"].str.len() != 2, \"uf\"] = pd.NA\n",
    "\n",
    "    df[\"ano_compra\"] = _to_int(df[\"ano_compra\"])\n",
    "    df[\"source_year\"] = _to_int(df[\"source_year\"])\n",
    "    df[\"ingest_date\"] = pd.to_datetime(df[\"ingest_date\"], errors=\"coerce\").dt.date\n",
    "\n",
    "    df[\"compra_ts\"] = _to_dt(df[\"compra\"])\n",
    "    df[\"insercao_ts\"] = _to_dt(df[\"insercao\"])\n",
    "    df[\"compra_date\"] = df[\"compra_ts\"].dt.date\n",
    "\n",
    "    df[\"qtd_itens_comprados\"] = _to_int(df[\"qtd_itens_comprados\"])\n",
    "    df[\"preco_unitario\"] = _to_num(df[\"preco_unitario\"])\n",
    "    df[\"preco_total\"] = _to_num(df[\"preco_total\"])\n",
    "\n",
    "    # regras mínimas de qualidade\n",
    "    df = df[df[\"compra_date\"].notna()]\n",
    "    df = df[df[\"preco_unitario\"].notna() & (df[\"preco_unitario\"] > 0)]\n",
    "\n",
    "    # --- load BigQuery ---\n",
    "    bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"ano_compra\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"nome_instituicao\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"cnpj_instituicao\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"municipio_instituicao\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"uf\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"compra\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"insercao\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"codigo_br\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"descricao_catmat\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"unidade_fornecimento\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"generico\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"anvisa\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"modalidade_compra\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"tipo_compra\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"capacidade\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"unidade_medida\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"unidade_fornecimento_capacidade\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"cnpj_fornecedor\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"fornecedor\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"cnpj_fabricante\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"fabricante\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"qtd_itens_comprados\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"preco_unitario\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"preco_total\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"source_year\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"ingest_date\", \"DATE\"),\n",
    "        bigquery.SchemaField(\"source_object\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"load_ts_utc\", \"STRING\"),\n",
    "        # campos silver\n",
    "        bigquery.SchemaField(\"compra_ts\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"insercao_ts\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"compra_date\", \"DATE\"),\n",
    "    ]\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        time_partitioning=bigquery.TimePartitioning(field=\"compra_date\"),\n",
    "        clustering_fields=[\"uf\", \"codigo_br\"],\n",
    "    )\n",
    "\n",
    "    print(\"Carregando em:\", SILVER_TARGET)\n",
    "    job = bq.load_table_from_dataframe(df, SILVER_TARGET, job_config=job_config)\n",
    "    job.result()\n",
    "    print(\"OK:\", SILVER_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
